name: Performance Benchmarks

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'sugar/**'
      - 'tools/**'
      - '.github/workflows/benchmark.yaml'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
      fail-fast: false
    env:
      SUGAR_PK: ${{ secrets.SUGAR_PK }}
      SUGAR_RPC_URI_10: ${{ secrets.SUGAR_RPC_URI_10 }}
      SUGAR_RPC_URI_8453: ${{ secrets.SUGAR_RPC_URI_8453 }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Run quick verification
        id: quick-test
        run: |
          echo "::group::Quick Test Results"
          python tools/quick_test.py
          echo "::endgroup::"

      - name: Run focused benchmarks
        id: focused-benchmark
        run: |
          echo "::group::Benchmark Results (Python ${{ matrix.python-version }})"
          python tools/benchmarks.py | tee focused_benchmark_output_py${{ matrix.python-version }}.txt
          echo "::endgroup::"

      - name: Run cache impact demo
        id: cache-demo
        run: |
          echo "::group::Cache Impact Demonstration (Python ${{ matrix.python-version }})"
          python tools/cache_impact_demo.py | tee cache_demo_output_py${{ matrix.python-version }}.txt
          echo "::endgroup::"

      - name: Parse and format benchmark results
        id: parse-results
        run: |
          # Parse focused benchmark results into structured tables
          if [ -f focused_benchmark_output_py${{ matrix.python-version }}.txt ]; then
            echo "Parsing focused benchmark results for Python ${{ matrix.python-version }}..."
            python tools/parse_benchmark_results.py focused_benchmark_output_py${{ matrix.python-version }}.txt > benchmark_summary_py${{ matrix.python-version }}.md
            
            # Add Python version header
            sed -i '1i## 🐍 Python ${{ matrix.python-version }} Results\n' benchmark_summary_py${{ matrix.python-version }}.md
          else
            echo "## ⚠️ Benchmark Results (Python ${{ matrix.python-version }})" > benchmark_summary_py${{ matrix.python-version }}.md
            echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "Focused benchmark output not found. Check workflow logs for details." >> benchmark_summary_py${{ matrix.python-version }}.md
          fi
          
          # Add cache impact analysis if available
          if [ -f cache_demo_output_py${{ matrix.python-version }}.txt ]; then
            echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "## 🧪 Cache Impact Analysis" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "This demonstrates why we use fresh chain instances for accurate benchmarking:" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "\`\`\`" >> benchmark_summary_py${{ matrix.python-version }}.md
            sed -n '/🔄 Testing with REUSED chain instance/,/🆕 Testing with FRESH chain instances/p' cache_demo_output_py${{ matrix.python-version }}.txt >> benchmark_summary_py${{ matrix.python-version }}.md
            sed -n '/get_all_tokens (fresh #3)/,/ANALYSIS:/p' cache_demo_output_py${{ matrix.python-version }}.txt | head -n -1 >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "\`\`\`" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
            echo "**Key Insight:** Fresh instances provide consistent timing, while reused instances show artificially fast subsequent calls due to caching." >> benchmark_summary_py${{ matrix.python-version }}.md
          fi
          
          # Add environment info
          echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "---" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "**Benchmark Environment:**" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "- 🏃 Runner: Ubuntu Latest" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "- 🐍 Python: ${{ matrix.python-version }}" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "- ✨ Fresh chain instances for each method call" >> benchmark_summary_py${{ matrix.python-version }}.md
          echo "- 📅 Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> benchmark_summary_py${{ matrix.python-version }}.md

      - name: Comment PR with benchmark results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            
            let benchmarkResults = '';
            try {
              benchmarkResults = fs.readFileSync('benchmark_summary_py${{ matrix.python-version }}.md', 'utf8');
            } catch (error) {
              benchmarkResults = '## ⚠️ Benchmark Results (Python ${{ matrix.python-version }})\n\nBenchmark results could not be generated. Check the workflow logs for details.';
            }
            
            const body = `## 🚀 Performance Benchmark Results (Python ${{ matrix.python-version }})
            
            ${benchmarkResults}

            // Find existing benchmark comment for this Python version
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingComment = comments.find(comment => 
              comment.user.login === 'github-actions[bot]' && 
              comment.body.includes('🚀 Performance Benchmark Results (Python ${{ matrix.python-version }})')
            );
            
            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-python-${{ matrix.python-version }}
          path: |
            focused_benchmark_output_py${{ matrix.python-version }}.txt
            cache_demo_output_py${{ matrix.python-version }}.txt
            benchmark_summary_py${{ matrix.python-version }}.md
          retention-days: 30

  # Summary job that runs after all Python versions complete
  benchmark-summary:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request' && always()
    steps:
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-python-*
          merge-multiple: true

      - name: Create combined summary comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            // Read all benchmark summary files
            const pythonVersions = ['3.9', '3.10', '3.11', '3.12'];
            let combinedResults = '## 🚀 Performance Benchmark Results - All Python Versions\n\n';
            
            for (const version of pythonVersions) {
              const filename = `benchmark_summary_py${version}.md`;
              try {
                if (fs.existsSync(filename)) {
                  const content = fs.readFileSync(filename, 'utf8');
                  combinedResults += content + '\n\n---\n\n';
                } else {
                  combinedResults += `## ⚠️ Python ${version} Results\n\nBenchmark results not available. Check individual job logs.\n\n---\n\n`;
                }
              } catch (error) {
                combinedResults += `## ⚠️ Python ${version} Results\n\nError reading results: ${error.message}\n\n---\n\n`;
              }
            }

            // Find existing summary comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const existingSummary = comments.find(comment => 
              comment.user.login === 'github-actions[bot]' && 
              comment.body.includes('🚀 Performance Benchmark Results - All Python Versions')
            );
            
            if (existingSummary) {
              // Update existing summary comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingSummary.id,
                body: combinedResults
              });
            } else {
              // Create new summary comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: combinedResults
              });
            }
