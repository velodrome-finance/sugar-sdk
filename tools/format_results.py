#!/usr/bin/env python3
"""
GitHub Actions compatible benchmark formatter.
Formats benchmark results for GitHub PR comments.
"""

import json
import sys
from datetime import datetime
from typing import Dict, List, Any

def format_benchmark_table(results: List[Dict[str, Any]]) -> str:
    """Format benchmark results as a markdown table"""
    
    # Group results by method
    methods = {}
    for result in results:
        method = result['method_name']
        if method not in methods:
            methods[method] = []
        methods[method].append(result)
    
    # Create table
    table = "| Method | Chain | Type | Time (s) | Relative Performance |\n"
    table += "|--------|-------|------|----------|---------------------|\n"
    
    for method_name, method_results in sorted(methods.items()):
        # Sort by execution time
        method_results.sort(key=lambda x: x['execution_time'])
        fastest_time = method_results[0]['execution_time']
        
        for i, result in enumerate(method_results):
            chain = result['chain_type']
            exec_type = result['execution_type']
            time_val = result['execution_time']
            
            if i == 0:
                relative = "ðŸ¥‡ Fastest"
            else:
                diff_pct = ((time_val - fastest_time) / fastest_time) * 100
                relative = f"+{diff_pct:.1f}% slower"
            
            table += f"| {method_name} | {chain} | {exec_type} | {time_val:.3f} | {relative} |\n"
        
        # Add separator between methods
        if method_name != list(methods.keys())[-1]:
            table += "|--------|-------|------|----------|---------------------|\n"
    
    return table


def format_async_vs_sync_comparison(results: List[Dict[str, Any]]) -> str:
    """Format async vs sync comparison tables"""
    
    # Group by chain and method
    chains = {}
    for result in results:
        chain = result['chain_type']
        if chain not in chains:
            chains[chain] = {}
        
        method = result['method_name']
        if method not in chains[chain]:
            chains[chain][method] = {}
        
        chains[chain][method][result['execution_type']] = result['execution_time']
    
    output = ""
    
    for chain_name, chain_data in sorted(chains.items()):
        output += f"\n### {chain_name} Chain - Async vs Sync Performance\n\n"
        output += "| Method | Async Time | Sync Time | Winner | Performance Difference |\n"
        output += "|--------|------------|-----------|---------|----------------------|\n"
        
        for method_name, method_data in sorted(chain_data.items()):
            if 'async' in method_data and 'sync' in method_data:
                async_time = method_data['async']
                sync_time = method_data['sync']
                
                if async_time < sync_time:
                    winner = "Async"
                    diff_pct = ((sync_time - async_time) / async_time) * 100
                    diff_text = f"{diff_pct:.1f}% faster"
                else:
                    winner = "Sync"
                    diff_pct = ((async_time - sync_time) / sync_time) * 100
                    diff_text = f"{diff_pct:.1f}% faster"
                
                output += f"| {method_name} | {async_time:.3f}s | {sync_time:.3f}s | {winner} | {diff_text} |\n"
    
    return output


def main():
    """Main function for GitHub Actions integration"""
    
    if len(sys.argv) < 2:
        print("Usage: python format_results.py <benchmark_output_file>")
        sys.exit(1)
    
    output_file = sys.argv[1]
    
    try:
        # Parse benchmark results from focused_benchmark output
        # This is a simplified parser - you could enhance it to parse JSON exports
        
        # For now, create sample data structure
        sample_results = [
            {"method_name": "get_all_tokens", "chain_type": "OP", "execution_type": "async", "execution_time": 0.253},
            {"method_name": "get_all_tokens", "chain_type": "OP", "execution_type": "sync", "execution_time": 0.234},
            {"method_name": "get_all_tokens", "chain_type": "Base", "execution_type": "async", "execution_time": 0.997},
            {"method_name": "get_all_tokens", "chain_type": "Base", "execution_type": "sync", "execution_time": 1.025},
            {"method_name": "get_pools", "chain_type": "OP", "execution_type": "async", "execution_time": 2.540},
            {"method_name": "get_pools", "chain_type": "OP", "execution_type": "sync", "execution_time": 2.261},
            {"method_name": "get_pools", "chain_type": "Base", "execution_type": "async", "execution_time": 8.028},
            {"method_name": "get_pools", "chain_type": "Base", "execution_type": "sync", "execution_time": 8.434},
        ]
        
        print("# ðŸŽ¯ Benchmark Results Summary")
        print()
        print("## ðŸ“Š Overall Performance Ranking")
        print()
        print(format_benchmark_table(sample_results))
        print()
        print("## âš¡ Async vs Sync Comparison")
        print(format_async_vs_sync_comparison(sample_results))
        print()
        print(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
        
    except Exception as e:
        print(f"Error formatting results: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
